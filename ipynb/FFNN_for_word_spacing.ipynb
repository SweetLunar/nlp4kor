{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이 문서는 비전공 초보가 제작하였으므로, 틀린 부분이 있을지도 모릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/bage/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py',\n",
       " '-f',\n",
       " '/run/user/1000/jupyter/kernel-8389058d-c667-4e15-b712-a9bca1b85bd2.json']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동영상과 다른 부분\n",
    "- 가장 마지막 \"check result\"부분에서, 동영상에서 학습된 문장이 사용되어, 새로운 문장으로 테스트하도록 수정.\n",
    "- 학습시간은 GTX 1080으로 수행하여 100만 문장이 약 17시간 소요됩니다. (left_gram=2, right_gram=2, layers=4)\n",
    "    - 동영상을 시연할 때는 GTX 1080Ti를 사용하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동영상 부연 설명\n",
    "- corpus에 존재하지 않은 음절(character)은 OneHotVector.to_vector()의 결과로 모든 값이 0인 벡터가 반환됩니다. \n",
    "    - 즉 characters에 없던 음절(문자)가 입력되는 경우, 모두 붙여쓰기됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설치 및 설정은 아래 페이지를 참고하여 주세요.\n",
    "- https://github.com/bage79/nlp4kor/blob/master/INSTALL.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다양한 유틸리티 클래스가 배포되었습니다.\n",
    "- https://github.com/bage79/nlp4kor/tree/master/bage_utils\n",
    "- 차후 강좌에서 계속 사용될 예정이오니, 아래에 import 되는 클래스들은 미리 한번 보시면 이해하시는데, 큰 도움이 되실 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFNN for 한글 띄어쓰기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/word_spacing.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 소스 코드: https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-09-2-xor-nn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 문자(음절) 데이터를 입력하기 위해 변환하려면? (text -> vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (방법1) 1d-array one-hot-vector (predefined dictionary)\n",
    "- 자음, 모음, 완성형, 영어/숫자/특수문자 의 모든 문자를 one hot vector 로 표시\n",
    "- vector 종류 수 = 11,316\n",
    "- 미리 vector 변환을 생성해 둘 수 있으나, vector의 크기가 매우 큼. 또한 한자등 다른 문자들은 제외됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bage_utils.hangul_util import HangulUtil # 한글처리\n",
    "from bage_utils.num_util import NumUtil # 숫자(int, str) 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자음: len: 30 ('ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ')\n",
      "모음: len: 21 ('ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ')\n",
      "완성형: len: 11172 ('가', '각', '갂', '갃', '간', '갅', '갆', '갇', '갈', '갉') ... ('힚', '힛', '힜', '힝', '힞', '힟', '힠', '힡', '힢', '힣')\n",
      "전체 한글(자음+모음+완성형) len: 11,223\n"
     ]
    }
   ],
   "source": [
    "print('자음:', 'len:', len(HangulUtil.JA_LIST), HangulUtil.JA_LIST)\n",
    "print('모음:', 'len:', len(HangulUtil.MO_LIST), HangulUtil.MO_LIST)\n",
    "print('완성형:', 'len:', len(HangulUtil.WANSUNG_LIST), \n",
    "      HangulUtil.WANSUNG_LIST[:10], '...', HangulUtil.WANSUNG_LIST[-10:])\n",
    "print('전체 한글(자음+모음+완성형)', 'len:', NumUtil.comma_str(len(HangulUtil.HANGUL_LIST)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 한글(자음+모음+완성형) + 영어 + 숫자 + 키보드특수문자: len: 11,317\n",
      "len(one-hot-vector): 11,317\n",
      "ㄱ 0 [1 0 0 ..., 0 0 0]\n",
      "ㅏ 30 [0 0 0 ..., 0 0 0]\n",
      "가 51 [0 0 0 ..., 0 0 0]\n",
      "힣 11222 [0 0 0 ..., 0 0 0]\n",
      "A 11249 [0 0 0 ..., 0 0 0]\n",
      "a 11223 [0 0 0 ..., 0 0 0]\n",
      "0 11275 [0 0 0 ..., 0 0 0]\n",
      "? 11316 [0 0 0 ..., 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print('전체 한글(자음+모음+완성형) + 영어 + 숫자 + 키보드특수문자:', 'len:', NumUtil.comma_str(len(HangulUtil.CHAR_LIST)))\n",
    "print('len(one-hot-vector):', NumUtil.comma_str(len(HangulUtil.to_one_hot_vector('ㄱ'))))\n",
    "print('ㄱ', HangulUtil.to_one_hot_index('ㄱ'), HangulUtil.to_one_hot_vector('ㄱ'))\n",
    "print('ㅏ', HangulUtil.to_one_hot_index('ㅏ'), HangulUtil.to_one_hot_vector('ㅏ'))\n",
    "print('가', HangulUtil.to_one_hot_index('가'), HangulUtil.to_one_hot_vector('가'))\n",
    "print('힣', HangulUtil.to_one_hot_index('힣'), HangulUtil.to_one_hot_vector('힣'))\n",
    "print('A', HangulUtil.to_one_hot_index('A'), HangulUtil.to_one_hot_vector('A'))\n",
    "print('a', HangulUtil.to_one_hot_index('a'), HangulUtil.to_one_hot_vector('a'))\n",
    "print('0', HangulUtil.to_one_hot_index('0'), HangulUtil.to_one_hot_vector('0'))\n",
    "print('?', HangulUtil.to_one_hot_index('?'), HangulUtil.to_one_hot_vector('?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (방법2) 1d-array non-one-hot-vector\n",
    "- 초성, 중성, 종성, 기타 순서로 3개의 vector를 생성한 후, 1d-array로 concate.\n",
    "- 한글이 아닌 경우에는 중성, 종성 부분은 항상 0.\n",
    "- 경우의 수 = (초성개수*중성개수*종성개수) + 영어개수 + 숫자개수 + 특수문자개수 + ....\n",
    "- 문자 종류별로 범위를 정의해 주어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초성: len: 19 ('ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ')\n",
      "중성: len: 21 ('ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ')\n",
      "종성: len: 28 ('', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ')\n",
      "영어: len: 52 ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "숫자: len: 10 ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "특수문자: len: 32 ['`', '~', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+', '-', '+', '[', ']', '\\\\', ';', \"'\", ',', '.', '/', '{', '}', '|', ':', '\"', '<', '>', '?']\n"
     ]
    }
   ],
   "source": [
    "print('초성:', 'len:', len(HangulUtil.CHO_LIST), HangulUtil.CHO_LIST)\n",
    "print('중성:', 'len:', len(HangulUtil.JUNG_LIST), HangulUtil.JUNG_LIST)\n",
    "print('종성:', 'len:', len(HangulUtil.JONG_LIST), HangulUtil.JONG_LIST)\n",
    "print('영어:', 'len:', len(HangulUtil.ENGLISH_LIST), HangulUtil.ENGLISH_LIST)\n",
    "print('숫자:', 'len:', len(HangulUtil.NUM_LIST), HangulUtil.NUM_LIST)\n",
    "print('특수문자:', 'len:', len(HangulUtil.KEYBOARD_SPECIAL_LIST), HangulUtil.KEYBOARD_SPECIAL_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 162\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "print('len:', len(HangulUtil.to_cho_jung_jong_vector('?')))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('각'))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('ㄱ'))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('a'))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('1'))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (방법3)  1d-array one-hot-vector (from corpus)\n",
    "- 한자 및 다른 문자들도 처리하고 싶고, 한글의 모든 문자를 항상 사용하는 것이 아님.\n",
    "- 따라서, 적당히 큰 말뭉치(corpus)에서 실제 사용되는 음절을 추출하여 사용.\n",
    "- 이 발표자료의 FFNN에서는 이 방법으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습용 말뭉치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: '/home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz'에 접근할 수 없습니다: 그런 파일이나 디렉터리가 없습니다\n",
      "gzip: /home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz: No such file or directory\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 총 문장 수 (dump corpus (ko.wikipedia.org) 문장 단위로 분리. 총 문장: 7,601,655 / 총 문서: 518,062)\n",
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz\n",
    "!zcat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz | wc -l # Ubuntu\n",
    "# !gzcat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz | wc -l # OSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: /home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!zcat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz | head # Ubuntu (Broken pipe error on only Jupyter)\n",
    "# !gzcat -cd ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz | head # OSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: '/home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters'에 접근할 수 없습니다: 그런 파일이나 디렉터리가 없습니다\n",
      "cat: /home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters: 그런 파일이나 디렉터리가 없습니다\n",
      "0\n",
      "cat: /home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters: 그런 파일이나 디렉터리가 없습니다\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters\n",
    "!cat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters | wc -l \n",
    "!cat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grep: /home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters: 그런 파일이나 디렉터리가 없습니다\n",
      "grep: /home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters: 그런 파일이나 디렉터리가 없습니다\n",
      "grep: /home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters: 그런 파일이나 디렉터리가 없습니다\n"
     ]
    }
   ],
   "source": [
    "!grep --color=never 한 ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters\n",
    "!grep --color=never 뷁 ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters\n",
    "!grep --color=never 쀓 ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters # not found in corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## character (from corpus) -> Vector \n",
    "x_data -> y_data => 로 표시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FFNN_for_word_spacing.001.jpeg\">\n",
    "<img src=\"./img/FFNN_for_word_spacing.002.jpeg\">\n",
    "<img src=\"./img/FFNN_for_word_spacing.003.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 (text_preprocessing.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log_path: /home/bage/anaconda3/lib/python3.5/site-packages/ipykernel/logs/__main__.py.-f.runuser1000jupyterkernel-8389058d-c667-4e15-b712-a9bca1b85bd2.json.log\n",
      "error_log_path: /home/bage/anaconda3/lib/python3.5/site-packages/ipykernel/logs/__main__.py.-f.runuser1000jupyterkernel-8389058d-c667-4e15-b712-a9bca1b85bd2.json.error.log\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from bage_utils.base_util import is_my_pc\n",
    "from bage_utils.datafile_util import DataFileUtil\n",
    "from bage_utils.dataset import DataSet\n",
    "from bage_utils.file_util import FileUtil\n",
    "from bage_utils.hangul_util import HangulUtil\n",
    "from bage_utils.mongodb_util import MongodbUtil\n",
    "from bage_utils.num_util import NumUtil\n",
    "from bage_utils.one_hot_vector import OneHotVector\n",
    "from nlp4kor.config import log, MONGO_URL, KO_WIKIPEDIA_ORG_SENTENCES_FILE, KO_WIKIPEDIA_ORG_CHARACTERS_FILE\n",
    "from nlp4kor.text_preprocess import TextPreprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 입력데이터(문장) 파일 생성 (Mongodb -> file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences_file: /home/bage/workspace/nlp4kor-ko.wikipedia.org/data/ko.wikipedia.org.sentences.gz\n"
     ]
    }
   ],
   "source": [
    "sentences_file = KO_WIKIPEDIA_ORG_SENTENCES_FILE\n",
    "log.info('sentences_file: %s' % sentences_file)\n",
    "if not os.path.exists(sentences_file):\n",
    "    TextPreprocess.dump_corpus(MONGO_URL, db_name='parsed', collection_name='ko.wikipedia.org', sentences_file=sentences_file,\n",
    "                               mongo_query={})  # mongodb -> text file(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 벡터매핑사전(음절) 파일 생성 (문장-> 음절)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters_file: /home/bage/workspace/nlp4kor-ko.wikipedia.org/dic/ko.wikipedia.org.characters\n"
     ]
    }
   ],
   "source": [
    "characters_file = KO_WIKIPEDIA_ORG_CHARACTERS_FILE\n",
    "log.info('characters_file: %s' % characters_file)\n",
    "if not os.path.exists(characters_file):\n",
    "    log.info('collect characters...')\n",
    "    TextPreprocess.collect_characters(sentences_file, characters_file)  # text file -> characters(unique features)\n",
    "    log.info('collect characters OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      \t unary\tbinary\tternary\n",
      "     0\t   [0]\t   [0]\t[1 0 0]\n",
      "     1\t   [0]\t   [1]\t[0 1 0]\n",
      "     2\t   [0]\t   [0]\t[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "unary_vector = OneHotVector([0])\n",
    "binary_vector = OneHotVector([0, 1])\n",
    "ternary_vector = OneHotVector([0, 1, 2])\n",
    "print('%6s\\t%6s\\t%6s\\t%6s' % ('', 'unary', 'binary', 'ternary'))\n",
    "for i in [0, 1, 2]:\n",
    "    print('%6s\\t%6s\\t%6s\\t%6s' % (i, unary_vector.to_vector(i), binary_vector.to_vector(i), ternary_vector.to_vector(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot vector for 음절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load characters list...\n",
      "load characters list OK. len: 17,380\n",
      "OneHotVector(len:17380) 17380\n",
      "OneHotVector(len:2) 2\n"
     ]
    }
   ],
   "source": [
    "log.info('load characters list...')\n",
    "features_vector = OneHotVector(DataFileUtil.read_list(characters_file))\n",
    "labels_vector = OneHotVector([0, 1])  # 붙여쓰기=0, 띄어쓰기=1\n",
    "log.info('load characters list OK. len: %s' % NumUtil.comma_str(len(features_vector))) # 데이터셋 마다 character 구성과 개수는 다름.\n",
    "\n",
    "print(features_vector, len(features_vector))\n",
    "print(labels_vector, len(labels_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [0 0 0 ..., 0 0 0] a 64\n",
      "b [0 0 0 ..., 0 0 0] b 65\n",
      "ㄱ [0 0 0 ..., 0 0 0] ㄱ 3129\n",
      "ㄲ [0 0 0 ..., 0 0 0] ㄲ 3130\n",
      "ㄳ [0 0 0 ..., 0 0 0] ㄳ 3131\n",
      "ㄴ [0 0 0 ..., 0 0 0] ㄴ 3132\n",
      "가 [0 0 0 ..., 0 0 0] 가 13891\n",
      "각 [0 0 0 ..., 0 0 0] 각 13892\n"
     ]
    }
   ],
   "source": [
    "for c in ['a', 'b', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', '가', '각']:\n",
    "    v = features_vector.to_vector(c) # one hot vector\n",
    "    _c = features_vector.to_value(v) # character for check\n",
    "    i = features_vector.to_index(c) # index number in characters list (0~17380)\n",
    "    print(c, v, _c, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset (training data, test data, validation data)\n",
    "- 너무 커서 공유하기 힘드네요.. 가장 큰 파일이 약 26GB\n",
    "- 직접 생성하시면 됩니다. ^^ (sentences, characters 파일 이용)\n",
    "\n",
    "##### ko.wikipedia.org.dataset.문장수.left_gram.right_gram.종류.gz\n",
    "- train: text, int 형식 (one-hot-vector로 저장하면 파일이 너무 커짐)\n",
    "- test, validation: one-hot-vector 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합계 4.0K\r\n",
      "drwxrwxr-x 2 bage bage 4.0K Jun 15 18:10 datasets\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합계 4.0K\r\n",
      "drwxrwxr-x 11 bage bage 4.0K Jun 14 23:09 word_spacing\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합계 36K\r\n",
      "drwxrwxr-x 2 bage bage 4.0K Jun  2 22:04 word_spacing_model.sentences=100.layers=2.left_gram=2.right_gram=2\r\n",
      "drwxrwxr-x 2 bage bage 4.0K Jun  2 22:02 word_spacing_model.sentences=100.layers=4.left_gram=2.right_gram=2\r\n",
      "drwxrwxr-x 2 bage bage 4.0K Jun  8 14:55 word_spacing_model.sentences=100.layers=4.left_gram=3.right_gram=3\r\n",
      "drwxrwxr-x 2 bage bage 4.0K Jun  2 22:02 word_spacing_model.sentences=10000.layers=2.left_gram=2.right_gram=2\r\n",
      "drwxrwxr-x 2 bage bage 4.0K Jun  2 22:02 word_spacing_model.sentences=10000.layers=4.left_gram=2.right_gram=2\r\n",
      "drwxrwxr-x 2 bage bage 4.0K Jun  3 16:30 word_spacing_model.sentences=1000000.layers=4.left_gram=2.right_gram=2\r\n",
      "drwxrwxr-x 2 bage bage 4.0K Jun  8 10:57 word_spacing_model.sentences=1000000.layers=4.left_gram=3.right_gram=3\r\n",
      "drwxrwxr-x 2 bage bage 4.0K Jun  6 19:50 word_spacing_model.sentences=1000000.layers=4.left_gram=4.right_gram=4\r\n",
      "drwxrwxr-x 2 bage bage 4.0K Jun 14 22:22 word_spacing_model.sentences=7601655.layers=4.left_gram=3.right_gram=3\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/models/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip. 제 노트북에 GPU가 없어요. GPU 있는 PC에서 개발해야 하나요?\n",
    "- Pycharm Deployment & Remote Interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각종 설정값 (word_spacing.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from bage_utils.base_util import is_my_pc\n",
    "from bage_utils.datafile_util import DataFileUtil\n",
    "from bage_utils.dataset import DataSet\n",
    "from bage_utils.datasets import DataSets\n",
    "from bage_utils.num_util import NumUtil\n",
    "from bage_utils.one_hot_vector import OneHotVector\n",
    "from bage_utils.watch_util import WatchUtil\n",
    "from nlp4kor.config import log, KO_WIKIPEDIA_ORG_WORD_SPACING_MODEL_DIR, KO_WIKIPEDIA_ORG_SENTENCES_FILE, KO_WIKIPEDIA_ORG_CHARACTERS_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sentences: 1000000\n",
      "layers: 4\n",
      "model_file: /home/bage/workspace/nlp4kor-ko.wikipedia.org/models/word_spacing/word_spacing_model.sentences=1000000.layers=4.left_gram=2.right_gram=2/model\n",
      "sentences_file: /home/bage/workspace/nlp4kor-ko.wikipedia.org/data/ko.wikipedia.org.sentences.gz\n",
      "characters_file: /home/bage/workspace/nlp4kor-ko.wikipedia.org/dic/ko.wikipedia.org.characters\n",
      "batch_size: 1000\n",
      "left_gram: 2, right_gram: 2\n",
      "ngram: 4\n",
      "features_vector: OneHotVector(len:17380)\n",
      "labels_vector: OneHotVector(len:2)\n",
      "n_features: 69520\n",
      "n_classes: 1\n",
      "n_hidden1: 100\n",
      "learning_rate: 0.01\n"
     ]
    }
   ],
   "source": [
    "if len(sys.argv) == 4:\n",
    "    max_sentences = int(sys.argv[1])\n",
    "    left_gram = int(sys.argv[2])\n",
    "    right_gram = int(sys.argv[3])\n",
    "else:\n",
    "    max_sentences, left_gram, right_gram = None, None, None\n",
    "\n",
    "if max_sentences is None:\n",
    "    max_sentences = int('1,000,000'.replace(',', '')) if is_my_pc() else int('1,000,000'.replace(',', ''))  # run 100 or 1M data (학습: 17시간 소요)\n",
    "    # max_sentences = 100 if is_my_pc() else FileUtil.count_lines(sentences_file, gzip_format=True) # run 100 or full data (학습시간: 5일 소요)\n",
    "if left_gram is None:\n",
    "    left_gram = 2\n",
    "if right_gram is None:\n",
    "    right_gram = 2\n",
    "\n",
    "layers = 4\n",
    "model_file = os.path.join(KO_WIKIPEDIA_ORG_WORD_SPACING_MODEL_DIR,\n",
    "                          'word_spacing_model.sentences=%s.layers=%s.left_gram=%s.right_gram=%s/model' % (max_sentences, layers, left_gram, right_gram))  # .%s' % max_sentences\n",
    "log.info('max_sentences: %s' % max_sentences)\n",
    "log.info('layers: %s' % layers)\n",
    "log.info('model_file: %s' % model_file)\n",
    "\n",
    "sentences_file = KO_WIKIPEDIA_ORG_SENTENCES_FILE\n",
    "log.info('sentences_file: %s' % sentences_file)\n",
    "\n",
    "characters_file = KO_WIKIPEDIA_ORG_CHARACTERS_FILE\n",
    "log.info('characters_file: %s' % characters_file)\n",
    "\n",
    "batch_size = 1000  # mini batch size\n",
    "ngram = left_gram + right_gram\n",
    "log.info('batch_size: %s' % batch_size)\n",
    "log.info('left_gram: %s, right_gram: %s' % (left_gram, right_gram))\n",
    "log.info('ngram: %s' % ngram)\n",
    "\n",
    "features_vector = OneHotVector(DataFileUtil.read_list(characters_file))\n",
    "labels_vector = OneHotVector([0, 1])  # 붙여쓰기=0, 띄어쓰기=1\n",
    "n_features = len(features_vector) * ngram  # number of features = 17,380 * 4\n",
    "n_classes = len(labels_vector) if len(labels_vector) >= 3 else 1  # number of classes = 2 but len=1\n",
    "log.info('features_vector: %s' % features_vector)\n",
    "log.info('labels_vector: %s' % labels_vector)\n",
    "log.info('n_features: %s' % n_features)\n",
    "log.info('n_classes: %s' % n_classes)\n",
    "\n",
    "n_hidden1 = 100\n",
    "learning_rate = 0.01  # 0.1 ~ 0.001\n",
    "log.info('n_hidden1: %s' % n_hidden1)\n",
    "log.info('learning_rate: %s' % learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test with samples. (word_spacing.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample testing...\n",
      "in : \"예쁜 운동화\"\n",
      "['예쁜', '쁜운', '운동', '동화'] -> [0, 1, 0, 0]\n",
      "out: \"예쁜 운동화\"\n",
      "in : \"즐거운 동화\"\n",
      "['즐거', '거운', '운동', '동화'] -> [0, 0, 1, 0]\n",
      "out: \"즐거운 동화\"\n",
      "in : \"삼풍동 화재\"\n",
      "['삼풍', '풍동', '동화', '화재'] -> [0, 0, 1, 0]\n",
      "out: \"삼풍동 화재\"\n",
      "sample testing OK.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlp4kor.ffnn.word_spacing import WordSpacing\n",
    "log.info('sample testing...')\n",
    "test_set = ['예쁜 운동화', '즐거운 동화', '삼풍동 화재']\n",
    "for s in test_set:\n",
    "    features, labels = WordSpacing.sentence2features_labels(s, left_gram=1, right_gram=1)\n",
    "    log.info('in : \"%s\"' % s)\n",
    "    log.info('%s -> %s' % (features, labels))\n",
    "    log.info('out: \"%s\"' % WordSpacing.spacing(s.replace(' ', ''), labels))\n",
    "log.info('sample testing OK.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력데이터 생성, 학습, 평가 (word_spacing.py)\n",
    "- WordSpacing.learning() in word_spacing.py\n",
    "- <u>내용이 너무 길어 실제 소스로 설명합니다.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(model_file + '.index') or not os.path.exists(model_file + '.meta'):\n",
    "    WordSpacing.learning(sentences_file, batch_size, left_gram, right_gram, model_file, features_vector, labels_vector, n_hidden1=n_hidden1, max_sentences=max_sentences, learning_rate=learning_rate, layers=layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그리고... \n",
    "- 지금까지 평가한 것은 4음절 단위에 대한 결과입니다. 하지만 우리가 원하는 것은 문장을 입력으로 했을 때의 결과죠.\n",
    "- 새로운 문장으로 제대로 띄어쓰기가 되는지 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chek result...\n",
      "skip 100000 th learned sentence.\n",
      "skip 200000 th learned sentence.\n",
      "skip 300000 th learned sentence.\n",
      "skip 400000 th learned sentence.\n",
      "skip 500000 th learned sentence.\n",
      "skip 600000 th learned sentence.\n",
      "skip 700000 th learned sentence.\n",
      "skip 800000 th learned sentence.\n",
      "skip 900000 th learned sentence.\n",
      "skip 1000000 th learned sentence.\n",
      "len(sentences): 100\n",
      "\n",
      "build_FFNN(layers=4)\n",
      "create tensorflow graph...\n",
      "n_features: 69520\n",
      "n_classes: 1\n",
      "n_hidden1: 100\n",
      "n_hidden2: 100\n",
      "n_hidden3: 100\n",
      "create tensorflow graph OK.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /home/bage/workspace/nlp4kor-ko.wikipedia.org/models/word_spacing/word_spacing_model.sentences=1000000.layers=4.left_gram=2.right_gram=2/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/bage/workspace/nlp4kor-ko.wikipedia.org/models/word_spacing/word_spacing_model.sentences=1000000.layers=4.left_gram=2.right_gram=2/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0] in : \"아버지가 방에 들어 가신다.\"\n",
      "[0] out: \"아버지가 방에 들어가 신다.\" (accuracy: 81.8%, sim: 66.7%=2/3)\n",
      "\n",
      "[1] in : \"가는 말이 고와야 오는 말이 곱다.\"\n",
      "[1] out: \"가는 말이고와야오는 말이 곱다.\" (accuracy: 84.6%, sim: 60.0%=3/5)\n",
      "\n",
      "[2] in : \"（화）히로시마 화물 터미널\"\n",
      "[2] out: \"（화） 히로시마 화물터미널\" (accuracy: 81.8%, sim: 50.0%=1/2)\n",
      "\n",
      "[3] in : \"서일본 여객철도： 산요 신칸센・산요 본선（이와쿠니 방면）・가베 선・구레 선 히로시마 전철：본선 (M1)\"\n",
      "[3] out: \"서일본 여객철도：산요 신칸센・산요 본선（이와 쿠니 방면）・가베선・구레선 히로시마 전철：본선(M1)\" (accuracy: 89.1%, sim: 60.0%=6/10)\n",
      "\n",
      "[4] in : \"교토의 마이코. 게이샤 (), 또는 게이기 ()는 예능에 종사하는 일본의 전통적인 기생이다.\"\n",
      "[4] out: \"교토의 마이코. 게이샤(), 또는 게이기()는 예능에 종사하는 일본의 전통적인 기생이다.\" (accuracy: 94.9%, sim: 81.8%=9/11)\n",
      "\n",
      "[5] in : \"이들은 전통 음악 연주, 전통 무용 공연, 작시와 같은 여러 가지 일본 예술에 능숙하다.\"\n",
      "[5] out: \"이들은 전통음악연주, 전통 무용 공연, 작시와 같은 여러 가지 일본 예술에 능숙하다.\" (accuracy: 94.3%, sim: 84.6%=11/13)\n",
      "\n",
      "[6] in : \"단순한 매춘부는 아니였지만, 요정이나 여관에 호출되어 시간을 정해 고객의 이야기 상대를 해주거나 노래나 춤으로 흥을 돋구는 일을 하고, 성매매를 하기도 했다.\"\n",
      "[6] out: \"단순한 매춘부는 아니였지만, 요정이나 여관에 호출되어 시간을 정해 고객의 이야기 상대를 해주거나 노래나 춤으로 흥을 돋구는 일을 하고, 성매매를 하기도 했다.\" (accuracy: 100.0%, sim: 100.0%=20/20)\n",
      "\n",
      "[7] in : \"\"게이샤\"라는 단어는 일본 한자로 \"예술\"을 뜻하는 (게이)와 \"사람\"을 뜻하는 (샤)로 이루어져 있으며, \"예술의 달인\"이라는 뜻이다.\"\n",
      "[7] out: \"\"게이샤\"라는 단어는 일본한 자로 \"예술\"을 뜻하는 (게이)와 \"사람\"을 뜻하는 (샤)로 이루어져 있으며, \"예술의 달인\"이라는 뜻이다.\" (accuracy: 96.7%, sim: 92.9%=13/14)\n",
      "\n",
      "[8] in : \"게이샤는 일본인에게 존경을 받으며, 정식 게이샤가 되기 위해서는 힘든 수련 과정을 거쳐야 한다.\"\n",
      "[8] out: \"게이샤는 일본인에게 존경을 받으며, 정식 게이 샤가 되기 위해서는 힘든 수련 과정을 거쳐야 한다.\" (accuracy: 97.5%, sim: 100.0%=12/12)\n",
      "\n",
      "[9] in : \"게이샤를 부르는 다른 말로는 게이코 ()가 있는데, 이는 교토에서 만들어져 그곳에서 쓰이는 말이다.\"\n",
      "[9] out: \"게이샤를 부르는 다른 말로는 게이코()가 있는데, 이는 교토에서 만들어져 그 곳에서 쓰이는 말이다.\" (accuracy: 95.2%, sim: 91.7%=11/12)\n",
      "\n",
      "[10] in : \"교토는 길고도 영향력 있는 게이샤 전통을 가진 도시로, 이곳에서 전문직 게이샤 (게이코)가 되기 위해서는 보통 5년의 수련을 거친다.\"\n",
      "[10] out: \"교토는 길고도 영향력 있는 게이 샤 전통을 가진 도시로, 이곳에서 전문직 게이샤(게이코)가 되기 위해서는 보통 5년의 수련을 거친다.\" (accuracy: 96.4%, sim: 94.1%=16/17)\n",
      "\n",
      "[11] in : \"견습 게이샤는 마이코 ()라고 부르며, 이 말은 일본 한자로 \"춤\"을 뜻하는 (마이)와 \"아이\"를 뜻하는 (코)가 합쳐진 단어이다.\"\n",
      "[11] out: \"견습 게이샤는 마이코()라고 부르며, 이 말은 일본한 자로 \"춤\"을 뜻하는 (마이)와 \"아이\"를 뜻하는 (코)가 합쳐진 단어이다.\" (accuracy: 94.6%, sim: 87.5%=14/16)\n",
      "\n",
      "[12] in : \"게이샤의 전통적인 모습으로 떠올리는 흰 화장과 다양하고 화려한 색상의 기모노 차림은 사실 마이코의 모습으로, 완전한 게이샤는 단순한 색상의 기모노를 입고 화장도 특별한 때에만 하얗게 칠한다.\"\n",
      "[12] out: \"게이샤의 전통적인 모습으로 떠올리는 흰화장과 다양하고 화려한 색상의 기모노 차림은 사실 마이코의 모습으로, 완전한 게이샤는 단순한 색상의 기모노를 입고화장도 특별한 때에만 하얗게 칠한다.\" (accuracy: 97.5%, sim: 91.7%=22/24)\n",
      "\n",
      "[13] in : \"다른 도시에도 게이샤가 있지만 교토와는 많이 다르다.\"\n",
      "[13] out: \"다른 도시에도 게이 샤가 있지만 교토와는 많이 다르다.\" (accuracy: 95.5%, sim: 100.0%=6/6)\n",
      "\n",
      "[14] in : \"도쿄에서는 정식 게이샤가 되기 위해 6개월에서 1년 정도만이 소요된다.\"\n",
      "[14] out: \"도쿄에서는 정식 게이 샤가 되기 위해 6개월에서 1년 정도만이 소요된다.\" (accuracy: 96.7%, sim: 100.0%=8/8)\n",
      "\n",
      "[15] in : \"도쿄의 견습 게이샤는 \"한교쿠\" (), 또는 술 접대부라는 뜻의 \"오샤쿠\" ()로 부른다.\"\n",
      "[15] out: \"도쿄 의 견습 게이샤는 \"한 교쿠\"(), 또는 술접대부라는 뜻의 \"오샤쿠\"()로 부른다.\" (accuracy: 86.8%, sim: 72.7%=8/11)\n",
      "\n",
      "[16] in : \"도쿄 게이샤는 교토 게이코보다 나이가 많은 것이 보통이다.\"\n",
      "[16] out: \"도쿄 게이샤는 교토 게이코보다 나이가 많은 것이 보통이다.\" (accuracy: 100.0%, sim: 100.0%=7/7)\n",
      "\n",
      "[17] in : \"현대 게이샤는 오늘날까지도 \"하나마치\" (, 꽃마을)에 지어진 \"오키야\"라는 전통 게이샤 저택에서 생활한다.\"\n",
      "[17] out: \"현대 게이샤는 오늘날까지도\"하나 마치\"(, 꽃마을)에 지어진 \" 오키야\"라는 전통 게이 샤저택에서 생활한다.\" (accuracy: 87.5%, sim: 72.7%=8/11)\n",
      "\n",
      "[18] in : \"그러나 나이를 먹은 성공한 게이샤들은 대부분 자신의 집을 소유하고 있다.\"\n",
      "[18] out: \"그러나 나 이를 먹은 성공한 게이 샤들은 대부분 자신의 집을 소유하고 있다.\" (accuracy: 93.3%, sim: 100.0%=9/9)\n",
      "\n",
      "[19] in : \"일본에서는 게이샤가 속하는 고상한 세계를 \"카류카이\" (, 화류계)라고 부른다.\"\n",
      "[19] out: \"일본에서는 게이 샤가속하는 고상 한 세계를 \"카류카이\"(, 화류계)라고 부른다.\" (accuracy: 88.6%, sim: 75.0%=6/8)\n",
      "\n",
      "[20] in : \"유명한 게이샤인 미네코 이와사키는 이 용어에 대해 \"게이샤는 꽃과 같이 아름다운 자신의 길을 지녔고, 버드나무처럼 공손하고 나긋나긋하며 강인하다.\"\n",
      "[20] out: \"유명한 게이샤인 미네 코이와사키는 이 용어에 대해 \"게이샤는 꽃과 같이 아름다운 자신의 길을 지녔고, 버드나무처럼 공손하고 나긋나긋하며 강인하다.\" (accuracy: 96.8%, sim: 94.1%=16/17)\n",
      "\n",
      "[21] in : \"\"라고 설명하였다.\"\n",
      "[21] out: \"\"라고 설명하였다.\" (accuracy: 100.0%, sim: 100.0%=1/1)\n",
      "\n",
      "[22] in : \"샤미센을 연주하는 게이샤, 1800년 기타가와 우타마로의 우키요에. 게이샤의 역사는 많은 사람들이 생각하는 것보다 그리 길지 않다.\"\n",
      "[22] out: \"샤미센을 연주하는 게이샤, 1800년 기타가와우타마로의 우키요에. 게이샤의 역사는 많은 사람들이 생각하는 것보다 그리길지 않다.\" (accuracy: 96.5%, sim: 86.7%=13/15)\n",
      "\n",
      "[23] in : \"게이샤가 등장하기 전인 헤이안 시대 (794년~1185년)부터 예능인으로 활동했던 여성은 있었으나, 진정한 게이샤는 그보다 훨씬 이후에 등장하였다.\"\n",
      "[23] out: \"게이 샤가 등장하기 전인 헤이안 시대(794년 ~ 1185년)부터 예능인으로 활동했던 여성은 있었으나, 진정한 게이샤는 그보다 훨씬 이후에 등장하였다.\" (accuracy: 93.9%, sim: 93.3%=14/15)\n",
      "\n",
      "[24] in : \"1589년, 도요토미 히데요시는 공창 제도의 일환으로 교토의 야나기초에 유곽을 건립하였고, 이 유곽은 후에 시마바라로 옮겨갔다.\"\n",
      "[24] out: \"1589년, 도요토미 히데요시는 공창제도의 일환으로 교토의 야나기초에 유곽을 건립하였고, 이유곽은 후에 시마바라로 옮겨갔다.\" (accuracy: 96.4%, sim: 85.7%=12/14)\n",
      "\n",
      "[25] in : \"유곽에서는 예술 향유, 음주, 고급 매춘이 이루어졌다.\"\n",
      "[25] out: \"유곽에서는 예술향유, 음주, 고급 매춘이 이루어졌다.\" (accuracy: 95.7%, sim: 83.3%=5/6)\n",
      "\n",
      "[26] in : \"유곽의 오이란 (花魁)들은 비싼 화대를 받고 매춘을 하였으며, 부유한 손님을 끌어들였다.\"\n",
      "[26] out: \"유곽의 오이란(花魁)들은 비싼 화대를 받고 매춘을 하였으며, 부유한 손님을 끌어들였다.\" (accuracy: 97.4%, sim: 90.0%=9/10)\n",
      "\n",
      "[27] in : \"많은 예능인들 또한 같은 장소에서 일을 하였으며, 음악과 무용, 작시로 손님을 즐겁게 해주었다.\"\n",
      "[27] out: \"많은 예능인들 또한 같은 장소에서 일을 하였으며, 음악과 무용, 작시로 손님을 즐겁게 해주었다.\" (accuracy: 100.0%, sim: 100.0%=12/12)\n",
      "\n",
      "[28] in : \"오랜 세월 동안 이 예능인들은 남자였으며, 그들 자신을 \"게이샤\" (예능인)나 \"호칸\" (어릿광대), \"다이코모치\" (일본 북 다이코를 치는 고수)로 불렀다.\"\n",
      "[28] out: \"오랜세월동안 이 예능인들은 남자였으며, 그들 자신을 \"게이샤\" (예능인)나 \"호칸\" (어릿광대), \"다이코모치\" (일본북 다이코를 치는 고수)로 불렀다.\" (accuracy: 95.7%, sim: 83.3%=15/18)\n",
      "\n",
      "[29] in : \"다른 일본 문화처럼 매춘의 세계도 매우 복잡해져갔다.\"\n",
      "[29] out: \"다른 일본 문화처럼 매춘의 세계도 매우 복잡해져갔다.\" (accuracy: 100.0%, sim: 100.0%=6/6)\n",
      "\n",
      "[30] in : \"오이란과 함께하고 싶은 남자들은 모두 까다로운 의식과 예법을 따라야 했고, 매우 부유하거나 귀족 계급인 사람만이 이것이 가능했다.\"\n",
      "[30] out: \"오이란과 함께 하고 싶은 남자들은 모두까다로운의식과 예법을 따라야 했고, 매우 부유하거나 귀족 계급 인 사람만이 이것이 가능했다.\" (accuracy: 92.7%, sim: 87.5%=14/16)\n",
      "\n",
      "[31] in : \"이러한 이유 때문에, 많은 찻집 (오차야)이 시마바라 근처에서 생겨났다.\"\n",
      "[31] out: \"이러한 이유 때문에, 많은 찻집(오차야)이시마바라 근처에서 생겨났다.\" (accuracy: 93.5%, sim: 75.0%=6/8)\n",
      "\n",
      "[32] in : \"일부 찻집에서는 여성들이 저렴하게 매춘을 하였는데, \"산차조로\"라고 불렀다.\"\n",
      "[32] out: \"일부 찻집에서는 여성들이 저렴하게 매춘을 하였는데, \"산차조로 \"라고 불렀다.\" (accuracy: 97.1%, sim: 100.0%=7/7)\n",
      "\n",
      "[33] in : \"그러나 \"오도로키\"라고 부르는 다른 여성들은 무용가와 연주가로 활동하였다.\"\n",
      "[33] out: \"그러나 \"오도로키\"라고 부르는 다른 여성들은 무용가와 연주가로 활동하였다.\" (accuracy: 100.0%, sim: 100.0%=7/7)\n",
      "\n",
      "[34] in : \"이 여성들은 등장한 지 얼마되지 않아 대중화되었고, 자신들을 시마바라에서 일하는 남성 예술가들처럼 \"게이샤\"라고 부르기 시작했다.\"\n",
      "[34] out: \"이 여성들은 등장한 지 얼마 되지 않아 대중화되었고, 자신들을 시마바라에서 일하는 남성예술가들처럼 \"게이샤\"라고 부르기 시작했다.\" (accuracy: 96.5%, sim: 92.9%=13/14)\n",
      "\n",
      "[35] in : \"1700년경이 되면서 게이샤는 남성보다 여성의 비율이 월등히 높아졌다.\"\n",
      "[35] out: \"1700년경이 되면서 게이샤는 남성보다 여성의 비율이월등히 높아졌다.\" (accuracy: 96.8%, sim: 85.7%=6/7)\n",
      "\n",
      "[36] in : \"몇 년 후에는 거의 모든 게이샤가 여성으로 이루어졌다.\"\n",
      "[36] out: \"몇 년 후에는 거의 모든 게이 샤가 여성으로 이루어졌다.\" (accuracy: 95.5%, sim: 100.0%=7/7)\n",
      "\n",
      "[37] in : \"조정에서는 게이샤가 매춘부로 활동하는 것을 금지하는 법을 제정하였고, 예능인으로만 활동할 수 있도록 허락하였다.\"\n",
      "[37] out: \"조정에서는 게이 샤가 매춘부로 활동하는 것을 금지하는 법을 제정하였고, 예능인으로만 활동할 수 있도록 허락하였다.\" (accuracy: 98.0%, sim: 100.0%=12/12)\n",
      "\n",
      "[38] in : \"이렇게 제정된 법 중에는 오비 (帯, 사시)를 뒤로 묶어서 기모노를 쉽게 벗는 것이 어렵도록 하는 법도 있었다.\"\n",
      "[38] out: \"이렇게 제정된 법중에는 오비(帯, 사시)를 뒤로 묶어서 기모노를 쉽게 벗는 것이 어렵도록 하는 법도 있었다.\" (accuracy: 95.6%, sim: 87.5%=14/16)\n",
      "\n",
      "[39] in : \"머리모양과 화장, 기모노 또한 오이란보다 단순해야만 했는데, 그들의 아름다움은 몸이 아닌 예술 행위에서 나와야 했기 때문이다.\"\n",
      "[39] out: \"머리 모양과 화장, 기모노 또한 오이란보다 단순해야만 했는데, 그들의 아름다움은 몸이 아닌 예술행위에서 나와야 했기 때문이다.\" (accuracy: 96.3%, sim: 93.3%=14/15)\n",
      "\n",
      "[40] in : \"이후 게이샤는 오이란보다 인기를 끌게 되었고, 1750년이 되면서 오이란은 사라졌다.\"\n",
      "[40] out: \"이후 게이샤는 오이란보다 인기를 끌게 되었고, 1750년이 되면서 오이란은 사라졌다.\" (accuracy: 100.0%, sim: 100.0%=9/9)\n",
      "\n",
      "[41] in : \"교토와 다른 도시에서는 새로운 게이샤 마을이 생겨났다.\"\n",
      "[41] out: \"교토와 다른 도시에서는 새로운 게이샤마을 이 생겨났다.\" (accuracy: 91.3%, sim: 83.3%=5/6)\n",
      "\n",
      "[42] in : \"19세기에 들어서 게이샤는 일반 여성보다 나은 지위를 갖게 되었으나, 일본 사회에서 문제가 되었다.\"\n",
      "[42] out: \"19세기에 들어서게이샤는 일반 여성보다 나은 지위를 갖게 되었으나, 일본사회에서 문제가 되었다.\" (accuracy: 95.2%, sim: 83.3%=10/12)\n",
      "\n",
      "[43] in : \"때때로 가난한 사람들이 자신의 딸을 하나마치 찻집으로 팔아 넘겼다.\"\n",
      "[43] out: \"때때로 가난한 사람들이 자신의 딸을 하나 마치 찻집으로 팔아 넘겼다.\" (accuracy: 96.4%, sim: 100.0%=8/8)\n",
      "\n",
      "[44] in : \"단나 (후원자)로 불리는 부유한 남성이 게이샤에게 많은 돈을 지불하여 개인적인 관심을 받기도 하였다.\"\n",
      "[44] out: \"단나(후원자)로 불리는 부유한 남성이 게이샤에게 많은 돈을 지불하여 개인적인 관심을 받기도 하였다.\" (accuracy: 97.7%, sim: 91.7%=11/12)\n",
      "\n",
      "[45] in : \"게이샤는 결혼을 하지 않았는데, 이 때문에 단나를 두어 지출을 충당해야만 했다.\"\n",
      "[45] out: \"게이샤는 결혼을 하지 않았는데, 이 때문에 단나를 두어지출을 충당해야만 했다.\" (accuracy: 97.0%, sim: 90.0%=9/10)\n",
      "\n",
      "[46] in : \"다른 남자들은 많은 돈을 지불하여 새로운 게이샤의 처녀성을 사는 경매 의식인 미즈아게를 치르기도 했다.\"\n",
      "[46] out: \"다른 남자들은 많은 돈을 지불하여 새로운 게이샤의 처녀성을 사는 경매의 식인 미즈아게를 치르기도 했다.\" (accuracy: 95.3%, sim: 92.3%=12/13)\n",
      "\n",
      "[47] in : \"게이샤에 대한 평판과 존중은 메이지 유신이 진행되면서 다시 상승하였고, 2차 세계 대전 이후에도 계속되었다.\"\n",
      "[47] out: \"게이샤에 대한 평판과 존중은 메이지 유신이 진행되면서 다시 상승하였고, 2차 세계 대전 이후에도 계속되었다.\" (accuracy: 100.0%, sim: 100.0%=13/13)\n",
      "\n",
      "[48] in : \"게이샤를 보호하는 법이 만들어지면서 더 이상 소녀들이 찻집에 팔리는 일이 없어졌고, 어린 게이샤의 처녀성이 팔리는 일도 사라졌다.\"\n",
      "[48] out: \"게이샤를 보호하는 법이 만들어지면서 더 이상 소녀들이 찻집에 팔리는 일이 없어졌고, 어린 게이샤의 처녀성이 팔리는 일도 사라졌다.\" (accuracy: 100.0%, sim: 100.0%=16/16)\n",
      "\n",
      "[49] in : \"이때부터 게이샤가 되고 싶어하는 여성만이 게이샤가 될 수 있게 되었다.\"\n",
      "[49] out: \"이때부터 게이 샤가 되고 싶어하는 여성만이 게이 샤가 될 수 있게 되었다.\" (accuracy: 93.1%, sim: 100.0%=9/9)\n",
      "\n",
      "[50] in : \"오늘날 게이샤의 활동 대부분은 교토 (특히 기온 하나마치)와 도쿄의 하나마치에서 이루어진다.\"\n",
      "[50] out: \"오늘날 게이샤의 활동 대부분은 교토(특히 기온 하나 마치)와 도쿄의 하나 마치에서 이루어진다.\" (accuracy: 92.5%, sim: 90.0%=9/10)\n",
      "\n",
      "[51] in : \"현대 일본에서 이들은 이 지역 이외에서 모습을 잘 드러내지 않는다.\"\n",
      "[51] out: \"현대 일본에서 이들은 이 지역 이외에서 모습을 잘 드러내지 않는다.\" (accuracy: 100.0%, sim: 100.0%=9/9)\n",
      "\n",
      "[52] in : \"1920년에는 80,000명이 넘는 게이샤가 일본에 있었으나, 오늘날에는 이보다 훨씬 적은 수만이 남아있다.\"\n",
      "[52] out: \"1920년에는 80,000명이 넘는 게이 샤가 일본에 있었으나, 오늘날에는 이보다 훨씬 적은 수만 이 남아있다.\" (accuracy: 95.8%, sim: 100.0%=11/11)\n",
      "\n",
      "[53] in : \"게이샤 감소에 대한 주요 원인은 서양 문화의 유입을 들 수 있다.\"\n",
      "[53] out: \"게이샤 감소에 대한 주요 원인은 서양문화의 유입을 들 수 있다.\" (accuracy: 96.0%, sim: 90.0%=9/10)\n",
      "\n",
      "[54] in : \"게이샤의 정확한 숫자에 대한 정보는 오늘날 알려져 있지 않지만, 대략 1,000명에서 2,000명 정도로 추정하고 있다.\"\n",
      "[54] out: \"게이샤의 정확한 숫자에 대한 정보는 오늘날 알려져 있지 않지만, 대략 1,000명에서 2,000명 정도로 추정하고 있다.\" (accuracy: 100.0%, sim: 100.0%=14/14)\n",
      "\n",
      "[55] in : \"관광객을 위해 게이샤로 나타나는 대부분의 여성들은 마이코로 분장한 또 다른 관광객이거나 배우인 경우가 많다.\"\n",
      "[55] out: \"관광객을 위해 게이샤로 나타나는 대부분의 여성들은 마이코로 분장한 또 다른 관광객이거나 배우인 경우가 많다.\" (accuracy: 100.0%, sim: 100.0%=13/13)\n",
      "\n",
      "[56] in : \"게이샤가 되고자 하는 어린 여성은 보통 중학교나 고등학교, 대학교를 졸업한 후 수련에 들어가며, 직업 활동은 어른이 된 이후에 시작한다.\"\n",
      "[56] out: \"게이 샤가 되고자 하는 어린 여성은 보통중학교 나고등학교, 대학교를 졸업한 후 수련에 들어가며, 직업 활동은 어른이된 이후에 시작한다.\" (accuracy: 91.2%, sim: 83.3%=15/18)\n",
      "\n",
      "[57] in : \"게이샤는 전통 가요와 샤미센, 샤쿠하치, 북과 같은 전통 악기 연주, 일본 전통 무용, 다도, 문학, 작시 등을 배운다.\"\n",
      "[57] out: \"게이샤는 전통가요와 샤미센, 샤쿠하치, 북과 같은 전통악기 연주, 일본 전통 무용, 다도, 문학, 작시 등을 배운다.\" (accuracy: 95.9%, sim: 88.2%=15/17)\n",
      "\n",
      "[58] in : \"견습생은 또한 다른 게이샤의 감독 아래 복장, 화장, 고객과의 거래처럼 까다로운 전통에도 숙련을 쌓아야 한다.\"\n",
      "[58] out: \"견습생은 또한 다른 게이샤의 감독 아래 복장, 화장, 고객과의 거래처럼까다로 운 전통에도 숙련을 쌓아야 한다.\" (accuracy: 95.7%, sim: 92.9%=13/14)\n",
      "\n",
      "[59] in : \"게이샤는 행사나 연회 같은 모임을 위해 고용되기도 하는데, 보통 찻집이나 전통 일본 식당에서 이루어지는 모임이다.\"\n",
      "[59] out: \"게이샤는 행사나 연회 같은 모임을 위해 고용되기도 하는데, 보통 찻집 이나 전통일본식당에서 이루어지는 모임이다.\" (accuracy: 93.8%, sim: 85.7%=12/14)\n",
      "\n",
      "[60] in : \"모임에서는 향을 태워서 고용 시간을 재는데, 이것에 따라 지불하는 비용을 \"센코다이\" (線香代, 선향대), 또는 \"교쿠다이\" (玉代, 옥대)라고 부른다.\"\n",
      "[60] out: \"모임에서는 향을 태워서 고용 시간을 재는데, 이것에 따라 지불하는 비용을 \"센코다이\"(線香代, 선향대), 또는 \"교쿠 다이\"(玉代, 옥대)라고 부른다.\" (accuracy: 95.5%, sim: 88.2%=15/17)\n",
      "\n",
      "[61] in : \"교토에서는 \"오하나\" (お花, 꽃), \"하나다이\" (花代, 화대)로 부른다.\"\n",
      "[61] out: \"교토에서는 \"오하나\"(お花, 꽃), \"하나 다이\"(花代, 화대)로 부른다.\" (accuracy: 91.2%, sim: 71.4%=5/7)\n",
      "\n",
      "[62] in : \"고객은 게이샤 조합 (検番, 겐반)을 통해 게이샤 출장 서비스를 이용할 수 있다.\"\n",
      "[62] out: \"고객은 게이샤 조합(検番, 겐반)을 통해 게이 샤 출장 서비스를 이용할 수 있다.\" (accuracy: 93.9%, sim: 90.9%=10/11)\n",
      "\n",
      "[63] in : \"게이샤 조합은 게이샤의 스케줄, 접대와 수련 일정을 관리한다.\"\n",
      "[63] out: \"게이샤 조합은 게이샤의 스케줄, 접대와 수련 일정을 관리한다.\" (accuracy: 100.0%, sim: 100.0%=7/7)\n",
      "\n",
      "[64] in : \"오비를 선보이는 세 명의 마이코. 전통적으로 게이샤는 매우 어린 나이부터 수련을 시작한다.\"\n",
      "[64] out: \"오비를 선보이는 세 명의 마이코. 전통적으로 게이샤는 매우 어린 나이부터 수련을 시작한다.\" (accuracy: 100.0%, sim: 100.0%=11/11)\n",
      "\n",
      "[65] in : \"과거에는 게이샤가 되기 위해 어린 나이에 팔려온 소녀들도 있었지만, 평판이 좋은 하나마치에서는 일반적인 일이 아니었다.\"\n",
      "[65] out: \"과거에는 게이 샤가 되기 위해 어린 나이에 팔려온 소녀들도 있었지만, 평판이 좋은 하나 마치에서는 일반적인 일이 아니었다.\" (accuracy: 96.1%, sim: 100.0%=14/14)\n",
      "\n",
      "[66] in : \"게이샤의 딸은 게이샤로 교육을 받는 경우가 흔했다.\"\n",
      "[66] out: \"게이샤의 딸은 게이샤로교육을 받는 경우가 흔했다.\" (accuracy: 95.2%, sim: 83.3%=5/6)\n",
      "\n",
      "[67] in : \"수련의 처음 단계는 \"시코미\"라고 부른다.\"\n",
      "[67] out: \"수련의 처음 단계는 \"시코미\"라고 부른다.\" (accuracy: 100.0%, sim: 100.0%=4/4)\n",
      "\n",
      "[68] in : \"과거에는 소녀가 처음 오키야 (찻집)에 도착했을 때 하녀로 일을 시작하거나 할 수 있는 모든 일을 했다.\"\n",
      "[68] out: \"과거에는 소녀가 처음 오키야(찻집)에 도착했을 때 하녀로 일을 시작하거나 할 수 있는 모든 일을 했다.\" (accuracy: 97.6%, sim: 93.3%=14/15)\n",
      "\n",
      "[69] in : \"이 일은 매우 고되었기 때문에 이 단계에서 수행을 그만두는 소녀도 많았다.\"\n",
      "[69] out: \"이 일은 매우고되었기 때문에 이 단계에서 수행을 그만두는 소녀도 많았다.\" (accuracy: 96.7%, sim: 90.0%=9/10)\n",
      "\n",
      "[70] in : \"저택의 연소자 시코미 대부분은 연장자 게이샤가 밤 늦게 일을 마치고 돌아올 때까지 기다려야했으며, 새벽 두 시나 세 시가 될 때까지 기다리기도 했다.\"\n",
      "[70] out: \"저택의 연소자 시코 미 대부분은 연장자 게이 샤가 밤 늦게 일을 마치고 돌아올 때까지 기다려야 했으며, 새벽두 시나세시가 될 때까지 기다리기도 했다.\" (accuracy: 90.2%, sim: 85.7%=18/21)\n",
      "\n",
      "[71] in : \"이 수련 단계 동안 시코미는 하나마치의 게이샤 학교에서 수업을 받았다.\"\n",
      "[71] out: \"이 수련 단계 동안 시 코미는 하나 마치의 게이 샤 학교에서 수업을 받았다.\" (accuracy: 89.7%, sim: 100.0%=9/9)\n",
      "\n",
      "[72] in : \"현대에 와서도 이 단계는 여전히 존재하지만, 과거에 비해 힘들지는 않다.\"\n",
      "[72] out: \"현대에 와서도 이 단계는 여전히 존재하지만, 과거에 비해 힘들지는 않다.\" (accuracy: 100.0%, sim: 100.0%=9/9)\n",
      "\n",
      "[73] in : \"또한, 오늘날의 시코미는 \"카류카이\" (화류계)의 전통과 복장을 따르게 되었다.\"\n",
      "[73] out: \"또한, 오늘날의 시코미는 \"카류카이\" (화류계)의 전통과 복장을 따르게 되었다.\" (accuracy: 100.0%, sim: 100.0%=8/8)\n",
      "\n",
      "[74] in : \"게이샤 예술을 완벽히 숙달하고 마지막으로 고난도의 무용 시험을 통과한 견습생은 수련의 두 번째 단계인 \"미나라이\"로 진급하였다.\"\n",
      "[74] out: \"게이샤예술을 완벽히 숙달하고 마지막으로 고난도의 무용 시험을 통과 한 견습생은 수련의 두 번째 단계인 \"미나라이\"로 진급하였다.\" (accuracy: 96.4%, sim: 93.3%=14/15)\n",
      "\n",
      "[75] in : \"미나라이부터는 더 이상 가사 노동을 하지 않았다.\"\n",
      "[75] out: \"미나라이부터는 더 이상 가사노동을 하지 않았다.\" (accuracy: 95.0%, sim: 83.3%=5/6)\n",
      "\n",
      "[76] in : \"이 단계 또한 오늘날 존재하는데, 과거에 비해 훨씬 기간이 짧다.\"\n",
      "[76] out: \"이 단계 또한 오늘 날 존재하는데, 과거에 비해 훨씬 기간이 짧다.\" (accuracy: 96.2%, sim: 100.0%=9/9)\n",
      "\n",
      "[77] in : \"(한 달) 미나라이는 현장에 직접 참여하여 배운다.\"\n",
      "[77] out: \"(한달) 미나라이는 현장에 직접 참여하여 배운다.\" (accuracy: 95.2%, sim: 83.3%=5/6)\n",
      "\n",
      "[78] in : \"이들은 연회에 찾아가지만 직접 직업 활동에 참여하지는 않고, 단순히 앉아서 오네상 (언니)에게 배움을 받거나 지켜보기만 한다.\"\n",
      "[78] out: \"이들은 연회에 찾아가지만 직접 직업 활동에 참여하지는 않고, 단순히 앉아서 오네상 (언니)에게 배움을 받거나 지켜보기만 한다.\" (accuracy: 100.0%, sim: 100.0%=15/15)\n",
      "\n",
      "[79] in : \"미나라이의 의상은 마이코의 것보다 화려한데, 이것이 그들의 수련 단계를 말해준다.\"\n",
      "[79] out: \"미나라이의의상은 마이코의 것보다 화려한데, 이것이 그들의 수련 단계를 말해준다.\" (accuracy: 97.1%, sim: 88.9%=8/9)\n",
      "\n",
      "[80] in : \"짧은 미나라이 기간이 끝난 후에는, 세 번째이자 가장 잘 알려진 단계인 \"마이코\" 수련을 시작한다.\"\n",
      "[80] out: \"짧은 미나라이기간이 끝난 후에는, 세 번째이자 가장 잘 알려진 단계인 \"마이코 \"수련을 시작한다.\" (accuracy: 92.7%, sim: 84.6%=11/13)\n",
      "\n",
      "[81] in : \"마이코는 견습 게이샤로서, 수련의 마지막 단계라고 할 수 있다.\"\n",
      "[81] out: \"마이코는 견습 게이샤로서, 수련의 마지막 단계라고 할 수 있다.\" (accuracy: 100.0%, sim: 100.0%=8/8)\n",
      "\n",
      "[82] in : \"마이코는 연장자 게이샤에게서 가르침을 받으며, 이들이 공연을 할 때는 수행을 하고자 따라간다.\"\n",
      "[82] out: \"마이코는 연장자 게이샤에게서 가르침을 받으며, 이들이 공연을 할 때는 수행을 하고자 따라 간다.\" (accuracy: 97.5%, sim: 100.0%=11/11)\n",
      "\n",
      "[83] in : \"오네상과 이모토상 (언니와 동생)의 관계는 매우 중요하다.\"\n",
      "[83] out: \"오네상과 이 모토상(언니와 동생)의 관계는 매우 중요하다.\" (accuracy: 92.0%, sim: 83.3%=5/6)\n",
      "\n",
      "[84] in : \"오니상은 하나마치에서 게이샤 활동에 대한 모든 것을 마이코에게 전수하는데, 차를 내오는 법, 샤미센을 연주하는 법, 무용을 비롯한 이키의 예술에 대한 모든 것을 가르쳐준다.\"\n",
      "[84] out: \"오니상은 하나 마치에서 게이샤 활동에 대한 모든 것을 마이코에게 전수하는데, 차를 내오는 법, 샤미센을 연주하는 법, 무용을 비롯한 이키의 예술에 대한 모든 것을 가르쳐 준다.\" (accuracy: 97.3%, sim: 100.0%=22/22)\n",
      "\n",
      "[85] in : \"마이코는 얼굴에 하얀 분을 두껍게 칠하고, 화려한 머리 장식을 하며, 거의 언제나 입술을 칠한다.\"\n",
      "[85] out: \"마이코는 얼굴에 하얀 분을 두껍게 칠하고, 화려한 머리 장식을 하며, 거의 언제나 입술을 칠한다.\" (accuracy: 100.0%, sim: 100.0%=13/13)\n",
      "\n",
      "[86] in : \"기모노와 오비는 정식 게이샤들이 입는 것에 비해 다양한 색깔이 많이 들어가며, 화려한 무늬도 수놓아져 있다.\"\n",
      "[86] out: \"기모노와 오비는 정식 게이 샤들이 입는 것에 비해 다양한 색깔이 많이 들어가며, 화려한 무늬도 수 놓아져 있다.\" (accuracy: 95.6%, sim: 100.0%=14/14)\n",
      "\n",
      "[87] in : \"미나라이와 마찬가지로, 마이코는 연회나 회합에 참석하여 많은 화대를 받을 수 없다.\"\n",
      "[87] out: \"미나라이와 마찬가지로, 마이코는 연회나 회합에 참석하여 많은 화대를 받을 수 없다.\" (accuracy: 100.0%, sim: 100.0%=10/10)\n",
      "\n",
      "[88] in : \"여섯 달 (도쿄), 또는 5년 (교토)의 수련 기간이 지난 후, 마이코는 정식 게이샤로 승격하게 되며, 일한 시간만큼의 충분한 화대를 지급 받을 수 있다.\"\n",
      "[88] out: \"여섯 달 (도쿄), 또는 5년(교토)의 수련 기간이 지난 후, 마이코는 정식 게이샤로 승격하게 되며, 일한 시간 만큼의 충분한화대를 지급받을 수 있다.\" (accuracy: 93.7%, sim: 86.4%=19/22)\n",
      "\n",
      "[89] in : \"게이샤는 색상이 적은 기모노를 입으며, 마이코일 때보다 성숙해졌기 때문에 일을 하거나 무용을 할 때만 화장을 하고, 자연미를 돋보이게 하고자 단순한 스타일을 취한다.\"\n",
      "[89] out: \"게이샤는 색상이 적은 기모노를 입으며, 마이코일 때보다 성숙해졌기 때문에 일을 하거나 무용을 할 때 만화장을 하고, 자연미를 돋보이게 하고 자 단순한 스타일을 취한다.\" (accuracy: 95.7%, sim: 95.2%=20/21)\n",
      "\n",
      "[90] in : \"게이샤는 보통 은퇴할 때까지 활동을 계속한다.\"\n",
      "[90] out: \"게이샤는 보통은퇴할 때까지 활동을 계속한다.\" (accuracy: 94.7%, sim: 80.0%=4/5)\n",
      "\n",
      "[91] in : \"게이샤의 추억\"\n",
      "[91] out: \"게이샤의 추억\" (accuracy: 100.0%, sim: 100.0%=1/1)\n",
      "\n",
      "[92] in : \"게이샤의 추억\"\n",
      "[92] out: \"게이샤의 추억\" (accuracy: 100.0%, sim: 100.0%=1/1)\n",
      "\n",
      "[93] in : \"게이샤의 추억(Memoirs of a Geisha)은 1997년에 출판된 아서 골든의 소설이다.\"\n",
      "[93] out: \"게이샤의 추억(Memoirs ofa Geisha)은 1997년에 출판된 아서 골든의 소설이다.\" (accuracy: 97.7%, sim: 88.9%=8/9)\n",
      "\n",
      "[94] in : \"2차 세계 대전 이전 일본의 교토를 무대로 하고 있으며, 게이샤로 살아가는 한 여자의 인생을 그린 이야기이다.\"\n",
      "[94] out: \"2차 세계 대전 이 전 일본의 교토를 무대로 하고 있으며, 게이샤로 살아가는 한 여자의 인생을 그린 이야기이다.\" (accuracy: 97.8%, sim: 100.0%=15/15)\n",
      "\n",
      "[95] in : \"2005년에 동명의 영화로 개봉되었다.\"\n",
      "[95] out: \"2005년에 동명의 영화로 개봉되었다.\" (accuracy: 100.0%, sim: 100.0%=3/3)\n",
      "\n",
      "[96] in : \"《게이샤의 추억》()은 아서 골든 동명의 소설을 각색해, 스티븐 스필버그가 제작한 2005년의 미국 서사 영화이다.\"\n",
      "[96] out: \"《게이샤의 추억》()은 아서 골든 동명의 소설을 각색해, 스티븐 스필버그가 제작한 2005년의 미국 서 사영화이다.\" (accuracy: 96.0%, sim: 92.3%=12/13)\n",
      "\n",
      "[97] in : \"이 영화는 롭 마셜이 감독을 맡았고, 미국에서는 2005년 12월 9일 개봉 되었고, 한국에서는 다음 해, 2006년 2월 2일에 개봉 되었다.\"\n",
      "[97] out: \"이 영화는 롭마셜이 감독을 맡았고, 미국에서는 2005년 12월 9일 개봉되었고, 한국에서는 다음해, 2006년 2월 2일에 개봉되었다.\" (accuracy: 93.3%, sim: 78.9%=15/19)\n",
      "\n",
      "[98] in : \"출연 배우로는 장쯔이, 와타나베 켄, 공리, 양자경, 쿠도 유키, 오고 스즈카 등 이 출연했다.\"\n",
      "[98] out: \"출연 배우로는 장쯔이, 와타나베 켄, 공리, 양자경, 쿠도 유키, 오고스즈카 등이 출연했다.\" (accuracy: 94.9%, sim: 84.6%=11/13)\n",
      "\n",
      "[99] in : \"주요 촬영은 캘리포니아 주 남부와 북부, 기요미즈 사원 및 후시미 이나리 신사를 포함한 교토의 여러 지역에서 촬영 되었다.\"\n",
      "[99] out: \"주요 촬영은 캘리포니아 주 남부와 북부, 기요미즈 사원 및 후시미이나리신사를 포함한 교토의 여러 지역에서 촬영되었다.\" (accuracy: 94.0%, sim: 82.4%=14/17)\n",
      "chek result OK.\n",
      "mean(accuracy): 95.73%, mean(sim): 90.99%\n",
      "secs/sentence: 0.1608\n",
      "average [00:00:16 0.0800] run tensorflow\n",
      "average [00:00:01 0.1361] read sentences\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.info('chek result...')\n",
    "watch = WatchUtil()\n",
    "watch.start('read sentences')\n",
    "\n",
    "sentences = ['아버지가 방에 들어 가신다.', '가는 말이 고와야 오는 말이 곱다.']\n",
    "max_test_sentences = 100\n",
    "with gzip.open(sentences_file, 'rt') as f:\n",
    "    if max_test_sentences < max_sentences:  # leared sentences is smaller than full sentences\n",
    "        for i, line in enumerate(f, 1):\n",
    "            if i <= max_sentences:  # skip learned sentences\n",
    "                if i % 100000 == 0:\n",
    "                    log.info('skip %d th learned sentence.' % i)\n",
    "                continue\n",
    "            if len(sentences) >= max_test_sentences:  # read new sentences\n",
    "                break\n",
    "\n",
    "            s = line.strip()\n",
    "            if s.count(' ') > 0 and len(s.replace(' ', '')) > ngram:  # sentence must have one or more space.\n",
    "                sentences.append(s)\n",
    "log.info('len(sentences): %s' % NumUtil.comma_str(len(sentences)))\n",
    "watch.stop('read sentences')\n",
    "\n",
    "watch.start('run tensorflow')\n",
    "accuracies, sims = [], []\n",
    "with tf.Session() as sess:\n",
    "    graph = WordSpacing.build_FFNN(n_features, n_classes, n_hidden1, learning_rate, layers=layers)\n",
    "    X, Y, predicted, accuracy = graph['X'], graph['Y'], graph['predicted'], graph['accuracy']\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    try:\n",
    "        restored = saver.restore(sess, model_file)\n",
    "    except:\n",
    "        log.error('restore failed. model_file: %s' % model_file)\n",
    "    try:\n",
    "        for i, s in enumerate(sentences):\n",
    "            log.info('')\n",
    "            log.info('[%s] in : \"%s\"' % (i, s))\n",
    "            features, labels = WordSpacing.sentence2features_labels(s, left_gram, right_gram)\n",
    "            dataset = DataSet(features=features, labels=labels, features_vector=features_vector, labels_vector=labels_vector)\n",
    "            dataset.convert_to_one_hot_vector()\n",
    "            if len(dataset) > 0:\n",
    "                _predicted, _accuracy = sess.run([predicted, accuracy], feed_dict={X: dataset.features, Y: dataset.labels})  # Accuracy report\n",
    "\n",
    "                generated_sentence = WordSpacing.spacing(s.replace(' ', ''), _predicted)\n",
    "                sim, correct, total = WordSpacing.sim_two_sentence(s, generated_sentence, left_gram=left_gram, right_gram=right_gram)\n",
    "\n",
    "                accuracies.append(_accuracy)\n",
    "                sims.append(sim)\n",
    "\n",
    "                log.info('[%s] out: \"%s\" (accuracy: %.1f%%, sim: %.1f%%=%s/%s)' % (i, generated_sentence, _accuracy * 100, sim * 100, correct, total))\n",
    "    except:\n",
    "        log.error(traceback.format_exc())\n",
    "\n",
    "log.info('chek result OK.')\n",
    "# noinspection PyStringFormat\n",
    "log.info('mean(accuracy): %.2f%%, mean(sim): %.2f%%' % (np.mean(accuracies) * 100, np.mean(sims) * 100))\n",
    "log.info('secs/sentence: %.4f' % (watch.elapsed('run tensorflow') / len(sentences)))\n",
    "log.info(watch.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip. 입력 데이터 증가, 파라미터 조절, 레이어 증가... 성능을 높이려면, 뭐 부터 해야 할까요?\n",
    "- 여러 가지 해보려면 시간이 적게 걸리는 것부터 해야 겠죠.\n",
    "- 파라미터 조절 or 레이어 증가 -> 입력 데이터 증가"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
